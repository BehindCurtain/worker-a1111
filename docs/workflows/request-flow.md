# Ä°ÅŸ SÃ¼reci: Request Flow

## Genel BakÄ±ÅŸ

Request Flow, bir RunPod API isteÄŸinin alÄ±nmasÄ±ndan gÃ¶rsel Ã¼retiminin tamamlanmasÄ±na kadar olan end-to-end sÃ¼reÃ§tir. Bu workflow, tÃ¼m sistem bileÅŸenlerinin koordineli Ã§alÄ±ÅŸmasÄ±nÄ± ve veri akÄ±ÅŸÄ±nÄ± tanÄ±mlar.

## SÃ¼reÃ§ AdÄ±mlarÄ±

### 1. Request Reception
**Sorumlu BileÅŸen**: RunPod Platform
**SÃ¼re**: < 100ms

#### Ä°ÅŸleyiÅŸ
- RunPod platform'dan serverless event alÄ±nÄ±r
- Event validation ve routing yapÄ±lÄ±r
- Handler function'a event iletilir

#### Veri YapÄ±sÄ±
```json
{
  "input": {
    "prompt": "a photograph of an astronaut riding a horse",
    "negative_prompt": "text, watermark, blurry, low quality",
    "steps": 25,
    "cfg_scale": 7,
    "width": 512,
    "height": 512,
    "sampler_name": "DPM++ 2M Karras"
  }
}
```

#### Kritik Noktalar
- Event format validation
- Input parameter extraction
- Handler function invocation

### 2. Handler Processing
**Sorumlu BileÅŸen**: handler.py - handler() function
**SÃ¼re**: < 10ms

#### Ä°ÅŸleyiÅŸ
```python
def handler(event):
    json = run_inference(event["input"])
    return json
```

#### Veri AkÄ±ÅŸÄ±
1. **Event Parsing**: RunPod event structure'dan input extraction
2. **Parameter Forwarding**: Input parametrelerini run_inference'a iletme
3. **Response Handling**: Inference sonucunu doÄŸrudan dÃ¶ndÃ¼rme

#### Hata SenaryolarÄ±
- Malformed event structure
- Missing input field
- Invalid parameter types

### 3. Service Readiness Check
**Sorumlu BileÅŸen**: handler.py - wait_for_service() function
**SÃ¼re**: 0-60 seconds (startup'da)

#### Ä°ÅŸleyiÅŸ
```python
def wait_for_service(url):
    while True:
        try:
            requests.get(url, timeout=120)
            return
        except:
            time.sleep(0.2)
```

#### Kontrol MekanizmasÄ±
- **Health Endpoint**: http://127.0.0.1:3000/sdapi/v1/sd-models
- **Polling Interval**: 0.2 seconds
- **Timeout**: 120 seconds per request
- **Retry Strategy**: Infinite loop until success

#### Service States
- **Not Ready**: WebUI API henÃ¼z baÅŸlatÄ±lmamÄ±ÅŸ
- **Starting**: WebUI API initialization sÃ¼reci
- **Ready**: API requests kabul etmeye hazÄ±r

### 4. Model Preparation & Hardcoded Checkpoint Management
**Sorumlu BileÅŸen**: handler.py - prepare_inference_request() function
**SÃ¼re**: 0-300 seconds (model download dependent)

#### Ä°ÅŸleyiÅŸ
```python
def prepare_inference_request(input_data):
    # 1. Validate request (no checkpoint validation needed - it's hardcoded)
    validate_request(input_data)
    
    # 2. Use hardcoded checkpoint and extract LoRAs from request
    checkpoint_info = HARDCODED_CHECKPOINT
    loras = input_data.get("loras", [])
    
    print(f"ğŸ¯ Using hardcoded checkpoint: {checkpoint_info['name']}")
    
    # 3. Prepare models (download if needed)
    checkpoint_path, lora_paths, models_downloaded = model_manager.prepare_models_for_request(
        checkpoint_info, loras
    )
    
    # 4. Handle checkpoint switching
    if checkpoint_info:
        current_model = get_current_model()
        target_checkpoint = checkpoint_info["name"]
        
        if target_checkpoint not in current_model:
            change_checkpoint(target_checkpoint)
            wait_for_model_loading()
            verify_checkpoint_loaded(target_checkpoint)
```

#### Hardcoded Checkpoint YÃ¶netimi
1. **Request Validation**: Sadece prompt kontrolÃ¼ (checkpoint hardcoded)
2. **Hardcoded Checkpoint**: Her zaman "Jib Mix Illustrious Realistic" kullanÄ±lÄ±r
3. **Current Model Check**: Mevcut yÃ¼klÃ¼ model kontrolÃ¼
4. **Checkpoint Switching**: API-based model deÄŸiÅŸimi (gerekirse)
5. **Loading Monitor**: Progress endpoint ile takip
6. **Verification**: Model deÄŸiÅŸiminin doÄŸrulanmasÄ±

#### Model Management
1. **Cache Check**: Verify if models are already cached
2. **Download Process**: Download missing models from CivitAI
3. **Registry Update**: Update model cache registry
4. **API Stability**: Verify WebUI API health after downloads

#### Checkpoint DeÄŸiÅŸim SÃ¼reci
- **Current Model Detection**: WebUI API'den mevcut model bilgisi
- **Target Comparison**: Hedef checkpoint ile karÅŸÄ±laÅŸtÄ±rma
- **API Request**: `/sdapi/v1/options` endpoint ile deÄŸiÅŸim
- **Progress Monitoring**: `/sdapi/v1/progress` ile takip
- **Verification**: Model deÄŸiÅŸiminin baÅŸarÄ±lÄ± olduÄŸunu doÄŸrulama
- **Error Handling**: BaÅŸarÄ±sÄ±zlÄ±k durumunda exception

#### Download Impact Handling
- **New Model Detection**: Track when models are downloaded
- **API Health Check**: Verify service stability post-download
- **Registry Sync**: 3-second wait for model registry updates
- **Graceful Degradation**: Continue with warnings if checks fail

### 5. Inference Request
**Sorumlu BileÅŸen**: handler.py - run_inference() function
**SÃ¼re**: 10-120 seconds

#### Ä°ÅŸleyiÅŸ
```python
def run_inference(inference_request):
    for attempt in range(max_retries):
        response = automatic_session.post(
            url=f'{LOCAL_URL}/txt2img',
            json=inference_request, 
            timeout=600
        )
        if response.status_code == 404:
            # Recovery mechanism
            wait_for_service(url=f'{LOCAL_URL}/sd-models')
            wait_for_txt2img_service()
```

#### HTTP Request Details
- **Method**: POST
- **URL**: http://127.0.0.1:3000/sdapi/v1/txt2img
- **Content-Type**: application/json
- **Timeout**: 600 seconds
- **Retry Policy**: 3 attempts with 5-second delays
- **Recovery Mechanism**: Automatic API health recovery on 404 errors

#### Request Processing
1. **Session Reuse**: HTTP connection pooling
2. **JSON Serialization**: Automatic parameter encoding
3. **Request Transmission**: HTTP POST to WebUI API
4. **Response Reception**: JSON response parsing
5. **Error Recovery**: 404 error recovery with API health checks

### 5. AI Processing
**Sorumlu BileÅŸen**: Automatic1111 WebUI API
**SÃ¼re**: 10-120 seconds (complexity dependent)

#### Processing Pipeline
1. **Prompt Processing**: Text encoding ve tokenization
2. **Noise Generation**: Random noise tensor creation
3. **Denoising Steps**: Iterative noise reduction (steps parameter)
4. **VAE Decoding**: Latent space to image conversion
5. **Post-processing**: Image formatting ve metadata

#### Resource Utilization
- **GPU Memory**: ~4-6GB (model + inference buffer)
- **Processing Time**: Steps ve resolution dependent
- **CPU Usage**: Minimal (GPU-accelerated processing)

#### Quality Factors
- **Steps**: Denoising iteration count (quality vs speed)
- **CFG Scale**: Prompt adherence strength
- **Sampler**: Algorithm selection impact
- **Resolution**: Output image dimensions

### 6. Response Generation
**Sorumlu BileÅŸen**: Automatic1111 WebUI API
**SÃ¼re**: < 1 second

#### Response Structure
```json
{
  "images": ["base64_encoded_image_data"],
  "parameters": {
    "prompt": "original_prompt",
    "steps": 25,
    "cfg_scale": 7,
    "width": 512,
    "height": 512,
    "sampler_name": "DPM++ 2M Karras",
    "seed": 1234567890
  },
  "info": "generation_metadata"
}
```

#### Response Components
- **Images**: Base64 encoded PNG data
- **Parameters**: Used generation parameters
- **Info**: Metadata ve generation details
- **Seed**: Reproducibility information

### 7. Response Forwarding
**Sorumlu BileÅŸen**: handler.py - run_inference() return
**SÃ¼re**: < 10ms

#### Ä°ÅŸleyiÅŸ
- WebUI API response'u JSON olarak parse edilir
- Response doÄŸrudan handler function'a dÃ¶ndÃ¼rÃ¼lÃ¼r
- Handler response'u RunPod platform'a iletir

#### Data Passthrough
- **No Processing**: Response modification yapÄ±lmaz
- **Direct Forward**: API response olduÄŸu gibi iletilir
- **Error Propagation**: API errors client'a yansÄ±tÄ±lÄ±r

### 8. Platform Response
**Sorumlu BileÅŸen**: RunPod Platform
**SÃ¼re**: < 100ms

#### Final Processing
- Handler response RunPod format'Ä±na wrap edilir
- Client'a HTTP response olarak iletilir
- Request lifecycle tamamlanÄ±r

## Veri AkÄ±ÅŸ DiyagramÄ±

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   RunPod        â”‚    â”‚    Handler       â”‚    â”‚   WebUI API     â”‚
â”‚   Platform      â”‚    â”‚    Service       â”‚    â”‚   Service       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â”‚ 1. Event              â”‚                       â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚                       â”‚
         â”‚                       â”‚ 2. Service Check      â”‚
         â”‚                       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
         â”‚                       â”‚ 3. Ready Response     â”‚
         â”‚                       â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
         â”‚                       â”‚ 4. Inference Request  â”‚
         â”‚                       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
         â”‚                       â”‚                       â”‚ 5. AI Processing
         â”‚                       â”‚                       â”‚ (10-120 seconds)
         â”‚                       â”‚ 6. Generated Response â”‚
         â”‚                       â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
         â”‚ 7. Final Response     â”‚                       â”‚
         â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                       â”‚
         â”‚                       â”‚                       â”‚
```

## Performance Metrikleri

### Latency Breakdown
- **Platform Overhead**: ~200ms (request + response)
- **Handler Processing**: ~20ms (parsing + forwarding)
- **Service Check**: 0ms (cached after startup)
- **AI Processing**: 10-120 seconds (dominant factor)
- **Total Latency**: 10-120 seconds + ~220ms overhead

### Throughput Characteristics
- **Sequential Processing**: One request at a time
- **GPU Bound**: Processing speed GPU performance dependent
- **Memory Limited**: VRAM capacity constrains batch size
- **Quality Trade-off**: Steps vs speed optimization

## Error Handling

### Error Propagation Chain
```
WebUI API Error â†’ HTTP Response â†’ run_inference() â†’ handler() â†’ RunPod Platform
```

### Common Error Scenarios

#### 1. Service Not Ready
- **Cause**: WebUI API not initialized
- **Handling**: wait_for_service() polling
- **Recovery**: Automatic retry until ready

#### 2. Invalid Parameters
- **Cause**: Malformed input parameters
- **Handling**: WebUI API validation
- **Response**: HTTP 400 with error details

#### 3. GPU Memory Exhaustion
- **Cause**: Large image generation request
- **Handling**: WebUI API error response
- **Recovery**: Request parameter adjustment needed

#### 4. Generation Timeout
- **Cause**: Complex prompt or high step count
- **Handling**: 600 second timeout
- **Recovery**: Request retry with reduced complexity

### Retry Mechanisms

#### HTTP Level Retries
- **Total Attempts**: 10
- **Backoff Factor**: 0.1 (exponential)
- **Status Codes**: 502, 503, 504
- **Timeout**: 600 seconds per attempt

#### Service Level Retries
- **Health Check**: Continuous polling
- **Interval**: 0.2 seconds
- **Logging**: Every 15 attempts

## Monitoring Points

### Request Tracking
1. **Request Reception**: RunPod event timestamp
2. **Handler Start**: Processing initiation
3. **Service Check**: Readiness validation
4. **Inference Start**: WebUI API request
5. **Processing Complete**: Response generation
6. **Response Sent**: Final response delivery

### Performance Metrics
- **End-to-End Latency**: Total request processing time
- **AI Processing Time**: Core generation duration
- **Queue Time**: Service readiness wait time
- **Error Rate**: Failed request percentage

### Health Indicators
- **Service Availability**: WebUI API uptime
- **Response Success Rate**: Successful generation percentage
- **Average Processing Time**: Performance trend tracking
- **Resource Utilization**: GPU ve memory usage

## Optimizasyon FÄ±rsatlarÄ±

### Latency Reduction
- **Model Caching**: Persistent VRAM storage
- **Parameter Optimization**: Default value tuning
- **Connection Pooling**: HTTP session reuse
- **Batch Processing**: Multiple request handling

### Throughput Improvement
- **Parallel Processing**: Multi-GPU support
- **Queue Management**: Request batching
- **Resource Scaling**: Dynamic resource allocation
- **Cache Strategies**: Response caching for similar requests

Bu workflow, projenin core functionality'sinin end-to-end iÅŸleyiÅŸini tanÄ±mlar ve tÃ¼m bileÅŸenlerin koordineli Ã§alÄ±ÅŸmasÄ±nÄ± saÄŸlar.
